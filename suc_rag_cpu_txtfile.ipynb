{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58608feb-7ce4-40af-96aa-a3c92854fadb",
      "metadata": {
        "id": "58608feb-7ce4-40af-96aa-a3c92854fadb"
      },
      "source": [
        "# Reranking\n",
        "\n",
        "<img src=\"./media/reranking.png\" width=600>\n",
        "\n",
        "[Image: Rerankers and Two-Stage Retrieval](https://www.pinecone.io/learn/series/rag/rerankers/)\n",
        "\n",
        "As semantic similarity becomes a core technique for delivering context to LLM-based applications, the challenge of **finding truly relevant information** grows more important. Most modern systems use **embedding models** to convert unstructured text into vector representations, storing these in a vector database for fast similarity-based retrieval.\n",
        "\n",
        "While this first-step retrieval process is efficient and scalable, **the top results may not always be the best-aligned passages for a given query.** They might be ‚Äúnear matches,‚Äù but not the most contextually relevant.\n",
        "This is where **reranking** comes in: a second-stage process that reorders the initially retrieved set to better match the true information need.\n",
        "\n",
        "Reranking uses a dedicated model‚Äîtypically a **cross-encoder** or a **late interaction model** to directly compare each candidate passage with the query, assigning a fine-grained relevance score. By re-evaluating these candidate passages, rerankers help surface  the most useful, specific, and accurate results to the top.\n",
        "\n",
        "In this notebook, we‚Äôll explore the most popular reranking approaches in modern RAG pipelines, with an intuitive look at how these models work and how they improve retrieval quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9ed77f-ef82-4bd1-9696-2388504d8e5d",
      "metadata": {
        "id": "dd9ed77f-ef82-4bd1-9696-2388504d8e5d"
      },
      "source": [
        "---\n",
        "## Reranking Models\n",
        "\n",
        "<img src=\"./media/embedding.png\" width=600>\n",
        "\n",
        "[Choosing the Right Embedding Model for RAG in Generative AI](https://medium.com/bright-ai/choosing-the-right-embedding-for-rag-in-generative-ai-applications-8cf5b36472e1)\n",
        "\n",
        "In a two stage RAG pipeline, we rely on a few different pre-trained encoder models to convert our unstructured content (generally text) into dense vector representations that capture the learned semantics of language through scaled machine learning. The first of which is the commonly known \"Embedding Model\" set up as a bi-encoder, one for the query one for the document(s). As this notebook is meant to focus on reranking, I will just provide a brief overview of base embedding models in this context, but if you'd like to learn the specifics you can check out [my guide on bidirectional encoder representations from transformers](https://www.youtube.com/watch?v=n_UQ0e0fBIA)!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcc3c3d-818c-4197-8af1-e00443244879",
      "metadata": {
        "id": "bfcc3c3d-818c-4197-8af1-e00443244879"
      },
      "source": [
        "### Context: Bi-Encoder (Embedding Model)\n",
        "\n",
        "<img src=\"./media/biencoder.png\" width=300>\n",
        "\n",
        "[Cross-encoders vs Bi-encoders : A deep-dive into text encoding methods](https://medium.com/@rbhatia46/cross-encoders-vs-bi-encoders-a-deep-dive-into-text-encoding-methods-d9aa890d6ca4)\n",
        "\n",
        "The **bi-encoder architecture** is widely used for vector similarity search in retrieval systems, where the base embedding model independently encodes queries and documents into numerical vectors (‚Äúembeddings‚Äù) that capture their meaning. At search time, these embeddings are quickly compared (usually via cosine similarity) to find the most relevant matches. This dual encoder architecture is where the \"bi\" from bi-encoder comes from, and typically uses models like [BERT](https://arxiv.org/pdf/1810.04805), which leverage the transformer architecture and attention mechanism to learn powerful language representations via objectives like masked language modeling (MLM). In MLM, the model is trained to predict missing words in sentences, allowing it to develop a deep understanding of context and semantics after training on many millions of examples, allowing the model to perform this dense vector encoding.\n",
        "\n",
        "As a quick clarification, the ‚Äúbi‚Äù in bi-encoder refers to using **two separate encoding passes** (one for queries, one for documents), often with shared model weights which is separate from BERT‚Äôs ‚Äúbidirectional‚Äù attention, which enables each token to attend to both left and right context during training.\n",
        "\n",
        "Let's take a quick look at what this looks like with a popular embedding model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad7c1d7-abd8-4493-b512-9c26f272b18b",
      "metadata": {
        "id": "4ad7c1d7-abd8-4493-b512-9c26f272b18b"
      },
      "source": [
        "#### Load the Model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "#### Define the Query & Documents\n",
        "query = \"Who is the best technical AI youtuber?\"\n",
        "\n",
        "documents = [\n",
        "    \"The dogwood is the state flower of North Carolina\",\n",
        "    \"Adam Lucek makes videos about artificial intelligence\",\n",
        "    \"The canada goose has a lifespan of 10-24 years\"]\n",
        "#### Example Encoding\n",
        "\n",
        "Here we can see what happens when the text is encoded into a numerical form. In the case of the embedding model we're using, our sentence will be mapped to a 384 dimensional dense vector space.\n",
        "query_embedding = embedding_model.encode(query)\n",
        "\n",
        "print(\"First 10 dimensions: \", query_embedding[:10])\n",
        "print(\"\\n Total Size: \",len(query_embedding), \"dimensions.\")\n",
        "#### Embedding Documents Independently\n",
        "doc_embeddings = embedding_model.encode(documents)\n",
        "#### Computing Similarity\n",
        "similarity = embedding_model.similarity(query_embedding, doc_embeddings)\n",
        "\n",
        "print(similarity)\n",
        "We can see that our query: *Who is the best technical AI youtuber?* is most similar to the embedding in position 2 (index 1): *Adam Lucek makes videos about artificial intelligence* üòé\n",
        "\n",
        "This is the backbone of bi-encoder based vector similarity based retrieval, converting and storing document embeddings then comparing them at run time to embedded queries. This done a lot more efficiently than our quick example using [vector databases](https://github.com/ALucek/embeddings-guide/blob/main/WTF_VDB.ipynb), but supports our first stage of the two stage RAG process- initial retrieval. Once our retrieved set is available, we can then employ our reranking models to provide a more refined ranking of relevant context for query responses.\n",
        "### Cross Encoder\n",
        "\n",
        "<img src=\"./media/cross-encoder.png\" width=800>\n",
        "\n",
        "[The Illustrated Guide to Cross-Encoders: From Deep to Shallow](https://medium.com/@kakumar1611/the-illustrated-guide-to-cross-encoders-from-deep-to-shallow-2a23a8630016)\n",
        "\n",
        "The **cross-encoder architecture** extends base embedding models by training them as classifiers for direct semantic similarity or relevance. Rather than computing document and query embeddings independently for later comparison, cross-encoders take the query and each document and **concatenate them as a single input** (e.g., `[CLS] query [SEP] document [SEP]`) to the model. The output is a **direct relevance score** for the query-document pair, typically produced by the model‚Äôs classification \\[CLS] token (or an output head attached to it).\n",
        "\n",
        "The key advantage of the cross-encoder is that **every token in the input (query and document) can attend to every other token**, allowing for richer, fine-grained interactions between query and document. This usually yields higher accuracy and more nuanced matching than bi-encoders. However, this comes at a computational cost as each query-document pair must be processed together, which is slower than similarity search with precomputed embeddings, hence why it's often applied as the second step in retrieval after a smaller candidate set has been retrieved.\n",
        "#### Load the Model\n",
        "\n",
        "For this example we'll be using the popular cross-encoder [ms-marco-MiniLM-L6-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
        "#### Define the Query & Documents\n",
        "query = \"What are the health benefits of meditation?\"\n",
        "\n",
        "documents = [\n",
        "    \"Several clinical studies have shown that regular meditation can help reduce stress and anxiety levels.\",\n",
        "    \"Meditation involves focusing the mind and eliminating distractions, often through breathing techniques or guided imagery.\",\n",
        "    \"A daily meditation practice has been associated with lower blood pressure and improved sleep quality in adults.\",\n",
        "    \"The city of Kyoto is famous for its Zen temples, where meditation has been practiced for centuries.\",\n",
        "    \"People who meditate frequently often report feeling calmer and more focused throughout the day.\",\n",
        "    \"Research suggests meditation may lower the risk of heart disease by reducing inflammation and improving heart rate variability.\",\n",
        "    \"Meditation apps have become increasingly popular, offering guided sessions on mindfulness and relaxation.\",\n",
        "    \"A 2021 meta-analysis found that meditation can reduce symptoms of depression when used alongside other treatments.\",\n",
        "    \"Some forms of meditation emphasize compassion and kindness, aiming to improve emotional well-being.\",\n",
        "    \"Athletes sometimes use meditation techniques to enhance concentration and mental resilience during competition.\",\n",
        "]\n",
        "#### Rank Documents\n",
        "ranks = cross_encoder.rank(query, documents)\n",
        "\n",
        "print(\"=\"*25, \"Cross Encoder Rankings\", \"=\"*25, \"\\n\")\n",
        "for rank in ranks:\n",
        "    print(f\"{rank['score']:.2f}\\t{documents[rank['corpus_id']]}\")\n",
        "As you can see we're able to order the passages a lot more closer to what's relevant to our query! Whereas a regular bi-encoder retrieval may focus too much on keywords and themes we're able to clearly extract the specifics.\n",
        "### Late Interaction Model\n",
        "\n",
        "<img src=\"./media/multi.png\" width=800>\n",
        "\n",
        "[ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/pdf/2004.12832)\n",
        "\n",
        "The late interaction architecture came after the popularization of using a cross-encoder with the release of ColBERT research and introduces a unique approach inspired by both the bi-encoder and cross-encoder setup. Late interaction based reranking takes the set of retrieved documents and query and splits each into their respective tokens. The query and each candidate document are then encoded as a matrix of token level vectors. This is opposed to the pooled single vector representation that is output by a base embedding model that combines all the token level embeddings together. Then for each query token the similarity is compared to all document token vectors and the maximum value is kept. This maximum similarity is aggregated across all query tokens to produce the final relevancy score for each document.\n",
        "<img src=\"./media/late-interaction.excalidraw.png\" width=800>\n",
        "\n",
        "Mathematically, given:\n",
        "\n",
        "- Query $Q$ with tokens $q_1, q_2, \\ldots, q_m$\n",
        "- Candidate document $D$ with tokens $d_1, d_2, \\ldots, d_n$\n",
        "\n",
        "Each token is encoded into a vector: $Q = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_m]$, $D = [\\mathbf{d}_1, \\mathbf{d}_2, \\ldots, \\mathbf{d}_n]$.\n",
        "\n",
        "For each query token $\\mathbf{q}_j$: $s_j = \\max_k \\left( \\cos (\\mathbf{q}_j, \\mathbf{d}_k) \\right)$\n",
        "\n",
        "The final document relevance score $S$ is aggregated by sum or mean:\n",
        "$S = \\sum_{j=1}^m s_j$, or $S = \\frac{1}{m} \\sum_{j=1}^m s_j$\n",
        "#### Ranking with ColBERT\n",
        "\n",
        "We'll be using [ColBERT V2](https://huggingface.co/colbert-ir/colbertv2.0) for our demonstration. Along with the model is an [official repo](https://github.com/stanford-futuredata/ColBERT/tree/main) that includes specific abstractions and functions meant to process documents and ranking with ColBERT and the proposed late interaction approach. But for demonstrations sake we'll implement late interaction directly with `transformers` and `torch`.\n",
        "#### Load the Model\n",
        "\n",
        "We'll use the [transformers](https://huggingface.co/docs/transformers/en/index) package directly to load the tokenizer and model from the ü§ó Hub.\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "colbert = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "#### Define the Query & Documents\n",
        "\n",
        "Same from before!\n",
        "query = \"What are the health benefits of meditation?\"\n",
        "\n",
        "documents = [\n",
        "    \"Several clinical studies have shown that regular meditation can help reduce stress and anxiety levels.\",\n",
        "    \"Meditation involves focusing the mind and eliminating distractions, often through breathing techniques or guided imagery.\",\n",
        "    \"A daily meditation practice has been associated with lower blood pressure and improved sleep quality in adults.\",\n",
        "    \"The city of Kyoto is famous for its Zen temples, where meditation has been practiced for centuries.\",\n",
        "    \"People who meditate frequently often report feeling calmer and more focused throughout the day.\",\n",
        "    \"Research suggests meditation may lower the risk of heart disease by reducing inflammation and improving heart rate variability.\",\n",
        "    \"Meditation apps have become increasingly popular, offering guided sessions on mindfulness and relaxation.\",\n",
        "    \"A 2021 meta-analysis found that meditation can reduce symptoms of depression when used alongside other treatments.\",\n",
        "    \"Some forms of meditation emphasize compassion and kindness, aiming to improve emotional well-being.\",\n",
        "    \"Athletes sometimes use meditation techniques to enhance concentration and mental resilience during competition.\",\n",
        "]\n",
        "#### Helper Functions\n",
        "\n",
        "We'll be defining two helper functions, `get_token_embeddings` and `colbert_score`.\n",
        "\n",
        "`get_token_embeddings` will compute the token-level embeddings through the model, taking in text and first running it through the tokenizer to split into individual tokens. Those tokens are then sent through the model to create a vector representation for each. We remove any special classifier or separator tokens that may have been output and keep just the individual embeddings.\n",
        "\n",
        "`colbert_score` then takes in the query embeddings and document embeddings, which have the shape of matrices:\n",
        "\n",
        "* The **query embeddings** have shape \\$(m, d)\\$, where \\$m\\$ is the number of query tokens and \\$d\\$ is the embedding dimension.\n",
        "* The **document embeddings** have shape \\$(n, d)\\$, where \\$n\\$ is the number of document tokens.\n",
        "\n",
        "The scoring function computes the **cosine similarity** between each query token embedding and every document token embedding, resulting in an \\$(m, n)\\$ similarity matrix. For each query token, we take the **maximum similarity** value across all document tokens. These maximum similarities are then **summed** across all query tokens to produce the final relevance score for the document.\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_token_embeddings(text, tokenizer, model):\n",
        "    # Get token-level embeddings, ignore [CLS] and [SEP]\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # outputs.last_hidden_state: (1, seq_len, hidden_dim)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
        "    \n",
        "    return outputs.last_hidden_state[0][keep_indices]  # (filtered_seq_len, hidden_dim)\n",
        "\n",
        "def colbert_score(query_emb, doc_emb):\n",
        "    \n",
        "    # query_emb: (m, d), doc_emb: (n, d)\n",
        "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)  # (m, n)\n",
        "    \n",
        "    max_sim, _ = sim.max(dim=1)  # (m,)\n",
        "    \n",
        "    return max_sim.sum().item()\n",
        "#### Rank Documents\n",
        "\n",
        "Now we put it all together by embedding the query and each document, computing the scores, then sorting the documents by relevance score\n",
        "# Compute token level embeddings\n",
        "query_emb = get_token_embeddings(query, colbert_tokenizer, colbert)\n",
        "doc_embs = [get_token_embeddings(doc, colbert_tokenizer, colbert) for doc in documents]\n",
        "\n",
        "# Compute scores\n",
        "scores = [colbert_score(query_emb, doc_emb) for doc_emb in doc_embs]\n",
        "\n",
        "ranking = sorted(zip(scores, documents), reverse=True)\n",
        "\n",
        "print(\"=\"*25, \"Late Interaction Rankings\", \"=\"*25, \"\\n\")\n",
        "for score, doc in ranking:\n",
        "    print(f\"{score:.2f}\\t{doc}\")\n",
        "<img src=\"./media/colbert_evals.png\" width=800>\n",
        "\n",
        "In the ColBERT research, they found that this late interaction approach provided comparable performance to existing cross encoders and other reranking methods but was quicker and required much less computation!\n",
        "## Putting it Together\n",
        "\n",
        "Now that we have an understanding of the models and approaches for reranking, let's create a simple RAG pipeline that can query and rerank results for a RAG response\n",
        "### Vector Database Setup\n",
        "#### Text Chunking\n",
        "\n",
        "For our database we'll do a simplified chunking setup on [The Adventures of Sherlock Holmes](https://www.gutenberg.org/ebooks/1661) as available through Project Gutenberg. We'll grab the text from the website and setup a very simple recursive token chunker that first splits the text into sentences, then combines into chunks of roughly 400 tokens long. These will be our candidate chunks for retrieval embedded into our vector database!\n",
        "import requests\n",
        "import nltk\n",
        "import tiktoken\n",
        "\n",
        "# Download Sherlock Holmes text\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Sentence split\n",
        "# Download punkt_tab sentence tokenizer from the natural language toolkit\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# Setup GPT-4 tokenizer @ 400 tokens target\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "token_limit = 400\n",
        "\n",
        "# Initiate chunking index and list\n",
        "chunks = []\n",
        "current_chunk = \"\"\n",
        "current_tokens = 0\n",
        "\n",
        "# Chunk\n",
        "for sentence in sentences:\n",
        "    sentence_tokens = len(enc.encode(sentence))\n",
        "    # If adding this sentence would go over the limit, start a new chunk\n",
        "    if current_tokens + sentence_tokens > token_limit:\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "        current_chunk = sentence\n",
        "        current_tokens = sentence_tokens\n",
        "    else:\n",
        "        if current_chunk:\n",
        "            current_chunk += \" \" + sentence\n",
        "        else:\n",
        "            current_chunk = sentence\n",
        "        current_tokens += sentence_tokens\n",
        "\n",
        "# Don't forget the last chunk!\n",
        "if current_chunk:\n",
        "    chunks.append(current_chunk)\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(\"\\nSample chunk:\\n\", chunks[10])\n",
        "#### VDB Initialization\n",
        "\n",
        "We'll use [ChromaDB](https://www.trychroma.com/) as our lightweight database of choice. ChromaDB by default relies on the embedding model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for embedding documents and queries.\n",
        "import chromadb\n",
        "\n",
        "# Instantiate the Chroma Client\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Create a Collection\n",
        "collection = chroma_client.get_or_create_collection(name=\"sherlock_holmes\")\n",
        "\n",
        "# Embed Chunks to the Collection\n",
        "collection.add(\n",
        "    documents=chunks,\n",
        "    ids=[str(i) for i in range(len(chunks))]\n",
        ")\n",
        "#### 1st Stage Retrieval\n",
        "\n",
        "We'll create a simple function to handle the initial retrieval from the collection, what's performed before reranking.\n",
        "def retrieve_docs(query, collection=\"sherlock_holmes\", n=25):\n",
        "    # Load Chroma Collection\n",
        "    collection = chroma_client.get_or_create_collection(name=collection)\n",
        "\n",
        "    # Perform semantic search\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n\n",
        "    )\n",
        "\n",
        "    # Zip documents and distances together into dicts\n",
        "    docs = results[\"documents\"][0]\n",
        "    scores = results[\"distances\"][0]\n",
        "\n",
        "    # Combine into list of dicts\n",
        "    return [{\"document\": doc, \"score\": score} for doc, score in zip(docs, scores)]\n",
        "#### 2nd Stage Reranking - Cross Encoder\n",
        "\n",
        "The first of two reranking functions, running our retrieved through a cross encoder, using the same as in our prior example.\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "def rerank_with_cross_encoder(query, results, cross_encoder_model=cross_encoder):\n",
        "    # Grab chunks from dictionary\n",
        "    documents = [r['document'] for r in results]\n",
        "    \n",
        "    # Compute cross encoder relevancy score\n",
        "    rerank_scores = cross_encoder_model.predict([(query, doc) for doc in documents])\n",
        "    \n",
        "    for r, score in zip(results, rerank_scores):\n",
        "        r['cross_encoder_score'] = float(score)\n",
        "    \n",
        "    # Sort results by cross_encoder_score, descending\n",
        "    results = sorted(results, key=lambda x: x['cross_encoder_score'], reverse=True)\n",
        "    \n",
        "    return results\n",
        "#### 2nd Stage Reranking - Late Interaction\n",
        "\n",
        "The second of our reranking functions, late interaction using ColBERT once more! We'll redefine our prior two helper functions again if this is being ran in isolation from the walkthrough code above.\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load ColBERTv2 model & tokenizer just once (at top level)\n",
        "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "def get_token_embeddings(text, tokenizer, model):\n",
        "    # Get token-level embeddings, ignore [CLS] and [SEP]\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
        "    return outputs.last_hidden_state[0][keep_indices]  # (filtered_seq_len, hidden_dim)\n",
        "\n",
        "def colbert_score(query_emb, doc_emb):\n",
        "    # query_emb: (m, d), doc_emb: (n, d)\n",
        "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)  # (m, n)\n",
        "    max_sim, _ = sim.max(dim=1)  # (m,)\n",
        "    return max_sim.sum().item()\n",
        "\n",
        "def rerank_with_late_interaction(query, results, tokenizer=colbert_tokenizer, model=colbert_model):\n",
        "    # Precompute query embeddings once\n",
        "    query_emb = get_token_embeddings(query, tokenizer, model)\n",
        "    for r in results:\n",
        "        doc_emb = get_token_embeddings(r['document'], tokenizer, model)\n",
        "        r['late_interaction_score'] = colbert_score(query_emb, doc_emb)\n",
        "    # Sort results by late_interaction_score descending\n",
        "    results = sorted(results, key=lambda x: x['late_interaction_score'], reverse=True)\n",
        "    return results\n",
        "### Testing it Out!\n",
        "\n",
        "Let's now run the entire pipeline and compare our outputs\n",
        "**Stage 1: Retrieval only**\n",
        "query = \"Show moments where Holmes or Watson reflect on friendship.\"\n",
        "\n",
        "# Stage 1: Retrieve top 50\n",
        "retrieved_docs = retrieve_docs(query, collection=\"sherlock_holmes\", n=50)\n",
        "\n",
        "print(\"=\"*20, \"Top 5 Retrieved (Semantic Search Only)\", \"=\"*20)\n",
        "for i, r in enumerate(retrieved_docs[:5]):\n",
        "    print(f\"[{i+1}] Score: {r['score']:.4f}\\n{r['document'][:200]}...\\n\")\n",
        "**Stage 2: Cross-Encoder reranking**\n",
        "cross_encoder_reranked = rerank_with_cross_encoder(query, retrieved_docs)\n",
        "\n",
        "print(\"=\"*20, \"Top 5 After Cross Encoder Rerank\", \"=\"*20)\n",
        "for i, r in enumerate(cross_encoder_reranked[:5]):\n",
        "    print(f\"[{i+1}] Cross-Encoder Score: {r['cross_encoder_score']:.2f}\\n{r['document'][:200]}...\\n\")\n",
        "**Stage 3: Late Interaction reranking**\n",
        "late_interaction_reranked = rerank_with_late_interaction(query, retrieved_docs)\n",
        "\n",
        "print(\"=\"*20, \"Top 5 After Late Interaction Rerank\", \"=\"*20)\n",
        "for i, r in enumerate(late_interaction_reranked[:5]):\n",
        "    print(f\"[{i+1}] Late Interaction Score: {r['late_interaction_score']:.2f}\\n{r['document'][:200]}...\\n\")\n",
        "### Combining with an LLM for RAG\n",
        "\n",
        "Now that we have our retrieval and reranking systems in place, we can now combine everything into a full fledged RAG system.\n",
        "from openai import OpenAI\n",
        "\n",
        "def rag_response(query, ranking=\"none\", k=5):\n",
        "\n",
        "    # Instantiate OpenAI Client\n",
        "    client = OpenAI()\n",
        "\n",
        "    # Retrieve our initial set of 50 documents\n",
        "    retrieved_documents = retrieve_docs(query, n=50)  # returns list of dicts with 'document' (and scores)\n",
        "\n",
        "    # Rerank and sort based on our three methods\n",
        "    if ranking == \"none\":\n",
        "        sorted_docs = sorted(retrieved_documents, key=lambda r: r['score'])\n",
        "        docs = [r['document'] for r in sorted_docs[:k]]\n",
        "    elif ranking == \"cross_encoder\":\n",
        "        reranked = rerank_with_cross_encoder(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    elif ranking == \"late_interaction\":\n",
        "        reranked = rerank_with_late_interaction(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    else:\n",
        "        raise ValueError(\"Argument 'ranking' must be one of ['none', 'cross_encoder', 'late_interaction']\")\n",
        "\n",
        "    # DEBUG PRINT: show which chunks are being provided\n",
        "    print(f\"\\nProvided Chunks (top {k}, ranking: {ranking}):\\n\" + \"-\"*60)\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"[{i}] {doc[:250]}{'...' if len(doc)>250 else ''}\\n\")  # Show first 250 chars for readability\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Combine top documents for passage to LLM\n",
        "    context = \"\\n\\n\".join(docs)\n",
        "    prompt = f\"\"\"You are a Sherlock Holmes expert. Use ONLY the following passages from the stories to answer the question.\n",
        "\n",
        "Passages:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "If you cannot find an answer in the passages, reply: \"The answer is not shown in the provided context.\" Otherwise, answer as specifically as possible using the text above.\n",
        "\"\"\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.5\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content.strip()\n",
        "**Simple Semantic Search**\n",
        "answer = rag_response(\"Show moments where Holmes or Watson reflect on friendship.\", ranking=\"none\")\n",
        "print(answer)\n",
        "**Reranking with Cross Encoder**\n",
        "answer = rag_response(\"Show moments where Holmes or Watson reflect on friendship.\", ranking=\"cross_encoder\")\n",
        "print(answer)\n",
        "**Reranking with Late Interaction**\n",
        "answer = rag_response(\"Show moments where Holmes or Watson reflect on friendship.\", ranking=\"late_interaction\")\n",
        "print(answer)\n",
        "---\n",
        "## Discussion\n",
        "\n",
        "In the above notebook we've demonstrated a two step retrieval and reranking process covering the tradition bi-encoder architecture for simple retrieval then cross-encoder and late interaction style setups and models for computing. The workings and findings of such are outlined in the below summary table:\n",
        "\n",
        "**Comparison Table**\n",
        "\n",
        "| Step         | Retrieval (Embedding)          | Rerank: Late Interaction (ColBERT)   | Rerank: Cross-Encoder           |\n",
        "| ------------ | ------------------------------ | ------------------------------------ | ------------------------------- |\n",
        "| Query Encode | 1 vector per query             | Matrix of vectors per query          | Joint encoding (query+doc pair) |\n",
        "| Doc Encode   | 1 vector per doc (pre-compute) | Matrix of vectors per doc (pre-comp) | N/A (encode per query+doc pair) |\n",
        "| Scoring      | Cosine/dot similarity          | MaxSim per query token + aggregate   | Full transformer, \\[CLS] output |\n",
        "| Compute cost | Very low                       | Moderate (matrix op per pair)        | High (full forward per pair)    |\n",
        "\n",
        "In essence, the bi-encoder is a recall-oriented coarse retrieval tool, while the cross-encoder and late interaction re-ranker is a precision-oriented fine reranking step. By relying on the traditional bi-encoder to quickly create a subset that can then be refined by a more compute intensive reranking model and improve retrieval results and downstream LLM generated contextual responses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers chromadb nltk torch beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78e8SEyYiiJ5",
        "outputId": "47d9f5aa-5e11-42b1-cdb4-63aae37545d2"
      },
      "id": "78e8SEyYiiJ5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.12-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.5)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.12-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.0.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=163d4224399bde58bb4d1d5fba2c60a9f491b2d561a2053ca031e3ad25bf5665\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-proto, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, nvidia-cusolver-cu12, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.2\n",
            "    Uninstalling starlette-0.46.2:\n",
            "      Successfully uninstalled starlette-0.46.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.12 coloredlogs-15.0.1 durationpy-0.10 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-instrumentation-0.55b1 opentelemetry-instrumentation-asgi-0.55b1 opentelemetry-instrumentation-fastapi-0.55b1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-util-http-0.55b1 overrides-7.7.0 posthog-5.0.0 pypika-0.48.9 python-dotenv-1.1.0 starlette-0.45.3 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIrFkZ3Pjnuh",
        "outputId": "87a28325-c160-40be-9bb5-474bdf2cb86b"
      },
      "id": "aIrFkZ3Pjnuh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load local models instead of cloud-based ones\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import chromadb\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from transformers import pipeline  # For local LLM\n",
        "import os\n",
        "\n",
        "# Download resources once\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# ======== LOCAL MODELS CONFIGURATION ========\n",
        "# Bi-encoder for embeddings\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Cross-encoder for reranking\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# ColBERT for late interaction\n",
        "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# Local LLM for generation (using Zephyr 7B Beta - smaller alternative: 'gpt2')\n",
        "local_llm = pipeline(\"text-generation\",\n",
        "                     model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "                     device_map=\"auto\",\n",
        "                     torch_dtype=torch.bfloat16)\n",
        "\n",
        "# ======== TEXT CHUNKING ========\n",
        "def prepare_documents():\n",
        "    \"\"\"Prepare documents for vector database\"\"\"\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    # Download Sherlock Holmes text\n",
        "    print(\"Downloading Sherlock Holmes text...\")\n",
        "    url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Sentence splitting\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # Simple chunking\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    token_limit = 400  # Target token count\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Approximate token count (1 token ~ 4 chars)\n",
        "        sentence_tokens = len(sentence) // 4\n",
        "        if len(current_chunk) + sentence_tokens > token_limit:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "            current_chunk = sentence\n",
        "        else:\n",
        "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# Initialize ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"sherlock_holmes\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# Add documents to ChromaDB if not already loaded\n",
        "if collection.count() == 0:\n",
        "    print(\"Populating vector database...\")\n",
        "    chunks = prepare_documents()\n",
        "    embeddings = embedding_model.encode(chunks).tolist()\n",
        "    collection.add(\n",
        "        documents=chunks,\n",
        "        embeddings=embeddings,\n",
        "        ids=[str(i) for i in range(len(chunks))]\n",
        "    )\n",
        "\n",
        "# ======== HELPER FUNCTIONS ========\n",
        "def get_token_embeddings(text, tokenizer, model):\n",
        "    \"\"\"Get token-level embeddings\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
        "    return outputs.last_hidden_state[0][keep_indices]\n",
        "\n",
        "def colbert_score(query_emb, doc_emb):\n",
        "    \"\"\"Compute ColBERT late interaction score\"\"\"\n",
        "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)\n",
        "    max_sim, _ = sim.max(dim=1)\n",
        "    return max_sim.sum().item()\n",
        "\n",
        "# ======== RETRIEVAL FUNCTIONS ========\n",
        "def retrieve_docs(query, n=25):\n",
        "    \"\"\"Initial retrieval from vector database\"\"\"\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n\n",
        "    )\n",
        "    return [\n",
        "        {\"document\": doc, \"score\": score}\n",
        "        for doc, score in zip(results['documents'][0], results['distances'][0])\n",
        "    ]\n",
        "\n",
        "def rerank_with_cross_encoder(query, results):\n",
        "    \"\"\"Rerank using cross-encoder\"\"\"\n",
        "    documents = [r['document'] for r in results]\n",
        "    rerank_scores = cross_encoder.predict([(query, doc) for doc in documents])\n",
        "    for r, score in zip(results, rerank_scores):\n",
        "        r['cross_encoder_score'] = float(score)\n",
        "    return sorted(results, key=lambda x: x['cross_encoder_score'], reverse=True)\n",
        "\n",
        "def rerank_with_late_interaction(query, results):\n",
        "    \"\"\"Rerank using late interaction\"\"\"\n",
        "    query_emb = get_token_embeddings(query, colbert_tokenizer, colbert_model)\n",
        "    for r in results:\n",
        "        doc_emb = get_token_embeddings(r['document'], colbert_tokenizer, colbert_model)\n",
        "        r['late_interaction_score'] = colbert_score(query_emb, doc_emb)\n",
        "    return sorted(results, key=lambda x: x['late_interaction_score'], reverse=True)\n",
        "\n",
        "# ======== LOCAL LLM RESPONSE GENERATION ========\n",
        "def generate_local_response(prompt):\n",
        "    \"\"\"Generate response using local LLM\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    response = local_llm(\n",
        "        messages,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=local_llm.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return response[0]['generated_text'][-1]['content']\n",
        "\n",
        "def rag_response(query, ranking=\"none\", k=5):\n",
        "    \"\"\"Full RAG pipeline with local models\"\"\"\n",
        "    # Retrieve initial documents\n",
        "    retrieved_documents = retrieve_docs(query, n=50)\n",
        "\n",
        "    # Apply selected reranking\n",
        "    if ranking == \"cross_encoder\":\n",
        "        reranked = rerank_with_cross_encoder(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    elif ranking == \"late_interaction\":\n",
        "        reranked = rerank_with_late_interaction(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    else:  # No reranking\n",
        "        docs = [r['document'] for r in retrieved_documents[:k]]\n",
        "\n",
        "    # Prepare context\n",
        "    context = \"\\n\\n\".join(docs)\n",
        "\n",
        "    # Create prompt for LLM\n",
        "    prompt = f\"\"\"Use the following passages to answer the question. If the answer isn't in the context, say so.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Generate response\n",
        "    return generate_local_response(prompt)\n",
        "\n",
        "# ======== EXAMPLE USAGE ========\n",
        "if __name__ == \"__main__\":\n",
        "    # Example query\n",
        "    query = \"Show moments where Holmes or Watson reflect on friendship.\"\n",
        "\n",
        "    print(\"\\n=== RAG Response with Semantic Search Only ===\")\n",
        "    print(rag_response(query, ranking=\"none\"))\n",
        "\n",
        "    print(\"\\n=== RAG Response with Cross-Encoder Reranking ===\")\n",
        "    print(rag_response(query, ranking=\"cross_encoder\"))\n",
        "\n",
        "    print(\"\\n=== RAG Response with Late Interaction Reranking ===\")\n",
        "    print(rag_response(query, ranking=\"late_interaction\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965,
          "referenced_widgets": [
            "e136f3d025ce4763991d4cc1e65f6ca8",
            "2ad160e232454251b5c9697e14dbc296",
            "947c7d1434614f3cb94f0748b3268810",
            "ccae06879ecb43a38bd2161d5e9c75de",
            "df35847a75ba4b2d9949b6b8b67f614b",
            "1d43f2c7c734404baa5cfbb4d0642881",
            "79568d1129854e82a86373d34c380211",
            "1b3ca868c8dd4fef87b495bc7d0e1e07",
            "04fb696816b541bb986a46e6b9e1e65e",
            "5d601d5b523d4fb2a576aff1dd8a26ce",
            "6fc53934598a4f0dbd698df039e3c07a",
            "b116c6c49fba4cd68f1119d761518d6d",
            "b92da0dc70b24244b31f9098ae125fae",
            "cd95485c324b43f5ac673b6c76e79bb4",
            "8e0d8439c49a4e57b70797a5f7bcc932",
            "a078a780e2f24c40ae1e7fe377228e99",
            "702f00bbacda4acfb4ebf5b8c8467fac",
            "ced0b9e49f5046ebbf455e738f35e238",
            "d1a69a37383145ed8b95247b8e819a1a",
            "b4913fc42ae64a3cb215536e0fb4c0c7",
            "7fddb9d7408e4363bc2a00c42a472829",
            "0cc5b82eb12840d6a0991f1f1f80b855",
            "b78af48c25e84b18a6b2611ff4134f23",
            "4a16023bcfa94c0ca5aba222dd80c9bf",
            "db0b6cba8269404aa1ae0683f2c65014",
            "0193ca6d6af74d0a883a1ad7a2782ff1",
            "86986ba1e8ff4f93ac57833e52e379d8",
            "bceab284519b458bbe5b28e02a00ad68",
            "34f520a82abd4b4ab4474112c8051c3c",
            "323524c7db964f338efe25959dfdcba5",
            "dd908e53aebb4a46936b3d86abb00bda",
            "1aacfb69a64f499ea0fe5edca10f2bfd",
            "78062b09f15946db8104dcc13253ce13",
            "8b207c440458435abdae79587e2b6ab3",
            "342959cca23544daaf2bc0e18da8b39e",
            "c1a2f4d593924bd7ad01f958fd8ea2b0",
            "19f967b307774bf2947186a532dc47fa",
            "9be2adc0b9e3410c89d8f8dc1fc4e640",
            "2cd1e86925c9476dbd76f54176b28fb0",
            "1d7e3e934eb941358e3dc1ef18452404",
            "876a7446ad3d497a8d3ab8380a8c69f7",
            "ba1072c1945843d9850f02e194722aaa",
            "330cad69f5be4f95af011eaae7a16390",
            "322d23198e894028859bc998be22a11b",
            "2841f3e1871e4c14aaf9ef8f729dd963",
            "ff07c2a9e54e446ab4d16e04a649b92b",
            "b79f5d7439144c8fae076e653dc4ace2",
            "4a0c099bdd414c06b49b9679c07cb9b8",
            "fac4df09db8a476ab962267f917b3140",
            "5aec63b14d3a46ffb3cc6fd2f5de3a8e",
            "e59f7244190e457cb41ce43b5c90e298",
            "73a31aa0dc834f67a4d9ad60e53166fe",
            "27bd08b30c8c49b1acfaa43a88c336eb",
            "4652e998c2024182a903d49858e165c6",
            "ada1fed3b8b7430db7a7d498283eac96",
            "5ac06b46dfce45a7a7764042b4705139",
            "cff81a6d1b9b460f9e539c2d22c3ba98",
            "5cf5fb7afa3e4696af0a6b23b63a7d76",
            "b564ac9180d5432f9e664b216cf0c5a2",
            "d06132c3059f4d3384df2ea27e6d90cc",
            "24b83be3ea414ca899b4e59477377a18",
            "a25869acc86841f2a1511b4574ce3761",
            "a061f25eb82743428b7cd5ed927814a9",
            "977a73d9d7c443c09d50fd9913fb39f0",
            "6b4855a29c894736992ef7e981887338",
            "f1a0a86b7bcf4cec871ca0322211bdc0"
          ]
        },
        "id": "GqtdLUEcii7y",
        "outputId": "4b6ac5da-67b0-4d7b-f25c-d02e69091f2a"
      },
      "id": "GqtdLUEcii7y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e136f3d025ce4763991d4cc1e65f6ca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b116c6c49fba4cd68f1119d761518d6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b78af48c25e84b18a6b2611ff4134f23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b207c440458435abdae79587e2b6ab3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2841f3e1871e4c14aaf9ef8f729dd963"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ac06b46dfce45a7a7764042b4705139"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating vector database...\n",
            "Downloading Sherlock Holmes text...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3843102183>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Populating vector database...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     collection.add(\n",
            "\u001b[0;32m<ipython-input-4-3843102183>\u001b[0m in \u001b[0;36mprepare_documents\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Sentence splitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Simple chunking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ŸÜÿ∏ÿßŸÖ RAG ŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖÿßÿ∞ÿ¨ ŸÖÿ¨ÿßŸÜŸäÿ©\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "import chromadb\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ======== ÿ•ÿπÿØÿßÿØ ÿßŸÑÿ®Ÿäÿ¶ÿ© ========\n",
        "# ÿ™ŸÜÿ≤ŸäŸÑ ÿ≠ÿ≤ŸÖÿ© nltk ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"nltk punkt package already installed\")\n",
        "except LookupError:\n",
        "    print(\"Downloading nltk punkt package...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "# ======== ÿ™ŸáŸäÿ¶ÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ========\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ ŸÑŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ Cross-Encoder\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ ColBERT\n",
        "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ™ŸàŸÑŸäÿØ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÖÿ≠ŸÑŸä (ÿ®ÿØŸäŸÑ ÿ£ÿµÿ∫ÿ±: 'gpt2')\n",
        "local_llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "print(\"All models loaded successfully!\")\n",
        "\n",
        "# ======== ÿ•ÿπÿØÿßÿØ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ© ========\n",
        "def prepare_documents():\n",
        "    \"\"\"ÿ™ÿ≠ÿ∂Ÿäÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ŸÑŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ©\"\"\"\n",
        "    print(\"Downloading Sherlock Holmes text...\")\n",
        "    url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "\n",
        "    # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÜÿµ ŸÖŸÜ ÿßŸÑÿ±ÿ£ÿ≥ ŸàÿßŸÑÿ™ÿ∞ŸäŸäŸÑ\n",
        "    start_idx = text.find(\"ADVENTURE I. A SCANDAL IN BOHEMIA\")\n",
        "    end_idx = text.find(\"End of the Project Gutenberg\")\n",
        "    text = text[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else text\n",
        "\n",
        "    # ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ¨ŸÖŸÑ\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # ÿ™ÿ¨ŸÖŸäÿπ ÿßŸÑÿ¨ŸÖŸÑ ŸÅŸä ŸÖŸÇÿßÿ∑ÿπ\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    token_limit = 400  # ÿ≠ÿØ ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑŸÖÿ≥ÿ™ŸáÿØŸÅ\n",
        "    char_per_token = 4  # ÿ™ŸÇÿØŸäÿ±: 4 ÿ£ÿ≠ÿ±ŸÅ ŸÑŸÉŸÑ ÿ±ŸÖÿ≤\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿä\n",
        "        sentence_tokens = len(sentence) / char_per_token\n",
        "\n",
        "        if len(current_chunk) + sentence_tokens > token_limit:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "            current_chunk = sentence\n",
        "        else:\n",
        "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# ÿ™ŸáŸäÿ¶ÿ© ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ©\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"sherlock_holmes\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ŸÉŸÜ ŸÖŸàÿ¨ŸàÿØÿ©\n",
        "if collection.count() == 0:\n",
        "    print(\"Populating vector database...\")\n",
        "    chunks = prepare_documents()\n",
        "    embeddings = embedding_model.encode(chunks).tolist()\n",
        "    ids = [str(i) for i in range(len(chunks))]\n",
        "\n",
        "    collection.add(\n",
        "        documents=chunks,\n",
        "        embeddings=embeddings,\n",
        "        ids=ids\n",
        "    )\n",
        "    print(f\"Added {len(chunks)} documents to vector database\")\n",
        "else:\n",
        "    print(f\"Vector database already contains {collection.count()} documents\")\n",
        "\n",
        "# ======== ÿØŸàÿßŸÑ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ========\n",
        "def get_token_embeddings(text, tokenizer, model):\n",
        "    \"\"\"ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿßŸÑÿ±ŸÖŸàÿ≤\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    # ÿ™ÿ¨ÿßŸáŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑÿÆÿßÿµÿ© [CLS] Ÿà [SEP]\n",
        "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
        "    return outputs.last_hidden_state[0][keep_indices]\n",
        "\n",
        "def colbert_score(query_emb, doc_emb):\n",
        "    \"\"\"ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ColBERT\"\"\"\n",
        "    # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜ ŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ŸàŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ\n",
        "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)\n",
        "    # ÿ£ÿÆÿ∞ ÿ£ŸÇÿµŸâ ÿ™ÿ¥ÿßÿ®Ÿá ŸÑŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    max_sim, _ = sim.max(dim=1)\n",
        "    # ÿ¨ŸÖÿπ ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÇÿµŸàŸâ\n",
        "    return max_sim.sum().item()\n",
        "\n",
        "# ======== ÿØŸàÿßŸÑ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ Ÿàÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ ========\n",
        "def retrieve_docs(query, n=25):\n",
        "    \"\"\"ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ£ŸàŸÑŸä ŸÖŸÜ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\"\"\"\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n\n",
        "    )\n",
        "    return [\n",
        "        {\"document\": doc, \"score\": score}\n",
        "        for doc, score in zip(results['documents'][0], results['distances'][0])\n",
        "    ]\n",
        "\n",
        "def rerank_with_cross_encoder(query, results):\n",
        "    \"\"\"ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ Cross-Encoder\"\"\"\n",
        "    documents = [r['document'] for r in results]\n",
        "    # ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿßÿ™ ÿßŸÑÿ£ŸáŸÖŸäÿ© ŸÑŸÉŸÑ ŸÖÿ≥ÿ™ŸÜÿØ ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    rerank_scores = cross_encoder.predict([(query, doc) for doc in documents])\n",
        "    for r, score in zip(results, rerank_scores):\n",
        "        r['cross_encoder_score'] = float(score)\n",
        "    # ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ±ÿ¨ÿ©\n",
        "    return sorted(results, key=lambda x: x['cross_encoder_score'], reverse=True)\n",
        "\n",
        "def rerank_with_late_interaction(query, results):\n",
        "    \"\"\"ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ™ŸÅÿßÿπŸÑ ÿßŸÑŸÖÿ™ÿ£ÿÆÿ± (ColBERT)\"\"\"\n",
        "    # ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÑŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    query_emb = get_token_embeddings(query, colbert_tokenizer, colbert_model)\n",
        "    for r in results:\n",
        "        # ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÑŸÉŸÑ ŸÖÿ≥ÿ™ŸÜÿØ\n",
        "        doc_emb = get_token_embeddings(r['document'], colbert_tokenizer, colbert_model)\n",
        "        # ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸÅÿßÿπŸÑ ÿßŸÑŸÖÿ™ÿ£ÿÆÿ±\n",
        "        r['late_interaction_score'] = colbert_score(query_emb, doc_emb)\n",
        "    # ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ±ÿ¨ÿ©\n",
        "    return sorted(results, key=lambda x: x['late_interaction_score'], reverse=True)\n",
        "\n",
        "# ======== ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ========\n",
        "def generate_local_response(prompt, max_length=500):\n",
        "    \"\"\"ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ŸÖÿ≠ŸÑŸä\"\"\"\n",
        "    # ÿ®ŸÜÿßÿ° ÿ±ÿ≥ÿßÿ¶ŸÑ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ©\n",
        "    response = local_llm(\n",
        "        messages,\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=local_llm.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ©\n",
        "    return response[0]['generated_text'][-1]['content']\n",
        "\n",
        "def rag_response(query, ranking=\"none\", k=5):\n",
        "    \"\"\"ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÉÿßŸÖŸÑ\"\"\"\n",
        "    print(f\"\\nProcessing query: '{query}'\")\n",
        "\n",
        "    # 1. ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ£ŸàŸÑŸä\n",
        "    print(\"Retrieving initial documents...\")\n",
        "    retrieved_documents = retrieve_docs(query, n=50)\n",
        "\n",
        "    # 2. ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä)\n",
        "    if ranking == \"cross_encoder\":\n",
        "        print(\"Reranking with cross-encoder...\")\n",
        "        reranked = rerank_with_cross_encoder(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    elif ranking == \"late_interaction\":\n",
        "        print(\"Reranking with late interaction...\")\n",
        "        reranked = rerank_with_late_interaction(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    else:  # ÿ®ÿØŸàŸÜ ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ\n",
        "        docs = [r['document'] for r in retrieved_documents[:k]]\n",
        "\n",
        "    # 3. ÿ™ÿ≠ÿ∂Ÿäÿ± ÿßŸÑÿ≥ŸäÿßŸÇ\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "    # 4. ÿ®ŸÜÿßÿ° ÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä\n",
        "    prompt = f\"\"\"ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿ≥ÿ§ÿßŸÑ. ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ¨ÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÅŸä ÿßŸÑŸÖŸÇÿßÿ∑ÿπÿå ŸÇŸÑ \"ŸÑÿß ÿ£ÿπÿ±ŸÅ\".\n",
        "\n",
        "ÿßŸÑŸÖŸÇÿßÿ∑ÿπ:\n",
        "{context}\n",
        "\n",
        "ÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}\n",
        "\n",
        "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\"\"\"\n",
        "\n",
        "    # 5. ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©\n",
        "    print(\"Generating response...\")\n",
        "    return generate_local_response(prompt)\n",
        "\n",
        "# ======== ÿßŸÑŸàÿßÿ¨Ÿáÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ========\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ - ŸÖÿ∫ÿßŸÖÿ±ÿßÿ™ ÿ¥ÿ±ŸÑŸàŸÉ ŸáŸàŸÑŸÖÿ≤\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nÿ£ÿØÿÆŸÑ ÿ≥ÿ§ÿßŸÑŸÉ (ÿ£Ÿà ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿ•ŸÜŸáÿßÿ°): \")\n",
        "\n",
        "        if query.lower() in ['exit', 'quit', 'ÿÆÿ±Ÿàÿ¨']:\n",
        "            break\n",
        "\n",
        "        print(\"\\n1. ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\")\n",
        "        print(\"2. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Cross-Encoder)\")\n",
        "        print(\"3. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Late Interaction)\")\n",
        "        choice = input(\"ÿßÿÆÿ™ÿ± ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ®ÿ≠ÿ´ (1/2/3): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            response = rag_response(query, ranking=\"none\")\n",
        "        elif choice == '2':\n",
        "            response = rag_response(query, ranking=\"cross_encoder\")\n",
        "        elif choice == '3':\n",
        "            response = rag_response(query, ranking=\"late_interaction\")\n",
        "        else:\n",
        "            print(\"ÿßÿÆÿ™Ÿäÿßÿ± ÿ∫Ÿäÿ± ÿµÿ≠Ÿäÿ≠ÿå ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\")\n",
        "            response = rag_response(query, ranking=\"none\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\")\n",
        "        print(response)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "kjSgISBslGTE",
        "outputId": "0a38d2bf-6e1f-4cbd-bda4-c16e0cc2c58c"
      },
      "id": "kjSgISBslGTE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk punkt package already installed\n",
            "Loading models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All models loaded successfully!\n",
            "Populating vector database...\n",
            "Downloading Sherlock Holmes text...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-435834805>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Populating vector database...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-435834805>\u001b[0m in \u001b[0;36mprepare_documents\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ¨ŸÖŸÑ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# ÿ™ÿ¨ŸÖŸäÿπ ÿßŸÑÿ¨ŸÖŸÑ ŸÅŸä ŸÖŸÇÿßÿ∑ÿπ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ÿ¥ÿ∫ÿßŸÑ"
      ],
      "metadata": {
        "id": "07mKr08Dxd8H"
      },
      "id": "07mKr08Dxd8H"
    },
    {
      "cell_type": "code",
      "source": [
        "# ŸÜÿ∏ÿßŸÖ RAG ŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖÿßÿ∞ÿ¨ ŸÖÿ¨ÿßŸÜŸäÿ© - ÿßŸÑÿ•ÿµÿØÿßÿ± ÿßŸÑŸÖÿπÿØŸÑ\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "import chromadb\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ======== ÿ•ÿπÿØÿßÿØ ÿßŸÑÿ®Ÿäÿ¶ÿ© ========\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ ŸÑŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ Cross-Encoder\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ ColBERT\n",
        "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ™ŸàŸÑŸäÿØ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÖÿ≠ŸÑŸä (ÿ®ÿØŸäŸÑ ÿ£ÿµÿ∫ÿ±: 'gpt2')\n",
        "local_llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "print(\"All models loaded successfully!\")\n",
        "\n",
        "# ======== ÿ•ÿπÿØÿßÿØ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ© ========\n",
        "def split_sentences(text):\n",
        "    \"\"\"ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ¨ŸÖŸÑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ regex ÿ®ÿØŸàŸÜ ÿßŸÑÿ≠ÿßÿ¨ÿ© ŸÑŸÄ nltk\"\"\"\n",
        "    # ÿ™ŸÇÿ≥ŸäŸÖ ÿπŸÜÿØ ÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ™ÿ±ŸÇŸäŸÖ ŸÖÿπ ÿßŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ÿßŸÑÿßÿÆÿ™ÿµÿßÿ±ÿßÿ™\n",
        "    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "    return re.split(pattern, text)\n",
        "\n",
        "def prepare_documents():\n",
        "    \"\"\"ÿ™ÿ≠ÿ∂Ÿäÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ŸÑŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ©\"\"\"\n",
        "    print(\"Downloading Sherlock Holmes text...\")\n",
        "    url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "\n",
        "    # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÜÿµ ŸÖŸÜ ÿßŸÑÿ±ÿ£ÿ≥ ŸàÿßŸÑÿ™ÿ∞ŸäŸäŸÑ\n",
        "    start_idx = text.find(\"ADVENTURE I. A SCANDAL IN BOHEMIA\")\n",
        "    end_idx = text.find(\"End of the Project Gutenberg\")\n",
        "    text = text[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else text\n",
        "\n",
        "    # ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ¨ŸÖŸÑ\n",
        "    sentences = split_sentences(text)\n",
        "\n",
        "    # ÿ™ÿ¨ŸÖŸäÿπ ÿßŸÑÿ¨ŸÖŸÑ ŸÅŸä ŸÖŸÇÿßÿ∑ÿπ\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    token_limit = 400  # ÿ≠ÿØ ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑŸÖÿ≥ÿ™ŸáÿØŸÅ\n",
        "    char_per_token = 4  # ÿ™ŸÇÿØŸäÿ±: 4 ÿ£ÿ≠ÿ±ŸÅ ŸÑŸÉŸÑ ÿ±ŸÖÿ≤\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿä\n",
        "        sentence_tokens = len(sentence) / char_per_token\n",
        "\n",
        "        if len(current_chunk) + sentence_tokens > token_limit:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "            current_chunk = sentence\n",
        "        else:\n",
        "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# ÿ™ŸáŸäÿ¶ÿ© ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ©\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"sherlock_holmes\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ŸÉŸÜ ŸÖŸàÿ¨ŸàÿØÿ©\n",
        "if collection.count() == 0:\n",
        "    print(\"Populating vector database...\")\n",
        "    chunks = prepare_documents()\n",
        "    embeddings = embedding_model.encode(chunks).tolist()\n",
        "    ids = [str(i) for i in range(len(chunks))]\n",
        "\n",
        "    collection.add(\n",
        "        documents=chunks,\n",
        "        embeddings=embeddings,\n",
        "        ids=ids\n",
        "    )\n",
        "    print(f\"Added {len(chunks)} documents to vector database\")\n",
        "else:\n",
        "    print(f\"Vector database already contains {collection.count()} documents\")\n",
        "\n",
        "# ======== ÿØŸàÿßŸÑ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ========\n",
        "def get_token_embeddings(text, tokenizer, model):\n",
        "    \"\"\"ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿßŸÑÿ±ŸÖŸàÿ≤\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    # ÿ™ÿ¨ÿßŸáŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑÿÆÿßÿµÿ© [CLS] Ÿà [SEP]\n",
        "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
        "    return outputs.last_hidden_state[0][keep_indices]\n",
        "\n",
        "def colbert_score(query_emb, doc_emb):\n",
        "    \"\"\"ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ColBERT\"\"\"\n",
        "    # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜ ŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ŸàŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ\n",
        "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)\n",
        "    # ÿ£ÿÆÿ∞ ÿ£ŸÇÿµŸâ ÿ™ÿ¥ÿßÿ®Ÿá ŸÑŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    max_sim, _ = sim.max(dim=1)\n",
        "    # ÿ¨ŸÖÿπ ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÇÿµŸàŸâ\n",
        "    return max_sim.sum().item()\n",
        "\n",
        "# ======== ÿØŸàÿßŸÑ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ Ÿàÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ ========\n",
        "def retrieve_docs(query, n=25):\n",
        "    \"\"\"ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ£ŸàŸÑŸä ŸÖŸÜ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\"\"\"\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n\n",
        "    )\n",
        "    return [\n",
        "        {\"document\": doc, \"score\": score}\n",
        "        for doc, score in zip(results['documents'][0], results['distances'][0])\n",
        "    ]\n",
        "\n",
        "def rerank_with_cross_encoder(query, results):\n",
        "    \"\"\"ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ Cross-Encoder\"\"\"\n",
        "    documents = [r['document'] for r in results]\n",
        "    # ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿßÿ™ ÿßŸÑÿ£ŸáŸÖŸäÿ© ŸÑŸÉŸÑ ŸÖÿ≥ÿ™ŸÜÿØ ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    rerank_scores = cross_encoder.predict([(query, doc) for doc in documents])\n",
        "    for r, score in zip(results, rerank_scores):\n",
        "        r['cross_encoder_score'] = float(score)\n",
        "    # ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ±ÿ¨ÿ©\n",
        "    return sorted(results, key=lambda x: x['cross_encoder_score'], reverse=True)\n",
        "\n",
        "def rerank_with_late_interaction(query, results):\n",
        "    \"\"\"ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ™ŸÅÿßÿπŸÑ ÿßŸÑŸÖÿ™ÿ£ÿÆÿ± (ColBERT)\"\"\"\n",
        "    # ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÑŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    query_emb = get_token_embeddings(query, colbert_tokenizer, colbert_model)\n",
        "    for r in results:\n",
        "        # ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÑŸÉŸÑ ŸÖÿ≥ÿ™ŸÜÿØ\n",
        "        doc_emb = get_token_embeddings(r['document'], colbert_tokenizer, colbert_model)\n",
        "        # ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸÅÿßÿπŸÑ ÿßŸÑŸÖÿ™ÿ£ÿÆÿ±\n",
        "        r['late_interaction_score'] = colbert_score(query_emb, doc_emb)\n",
        "    # ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ±ÿ¨ÿ©\n",
        "    return sorted(results, key=lambda x: x['late_interaction_score'], reverse=True)\n",
        "\n",
        "# ======== ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ========\n",
        "def generate_local_response(prompt, max_length=500):\n",
        "    \"\"\"ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ŸÖÿ≠ŸÑŸä\"\"\"\n",
        "    # ÿ®ŸÜÿßÿ° ÿ±ÿ≥ÿßÿ¶ŸÑ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ©\n",
        "    response = local_llm(\n",
        "        messages,\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=local_llm.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ©\n",
        "    generated_text = response[0]['generated_text']\n",
        "\n",
        "    # ŸÅŸä ÿ®ÿπÿ∂ ÿßŸÑÿ£ÿ≠ŸäÿßŸÜ Ÿäÿ±ÿ¨ÿπ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ±ÿ≥ÿßÿ¶ŸÑ ŸÉÿßŸÖŸÑÿ©ÿå ŸÑÿ∞ŸÑŸÉ ŸÜÿ≥ÿ™ÿÆÿ±ÿ¨ ŸÅŸÇÿ∑ ÿßŸÑÿ±ÿØ ÿßŸÑÿ£ÿÆŸäÿ±\n",
        "    if \"assistant\" in generated_text:\n",
        "        # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ¨ÿ≤ÿ° ÿ®ÿπÿØ ÿ¢ÿÆÿ± \"assistant\"\n",
        "        parts = generated_text.split(\"assistant\")\n",
        "        if len(parts) > 1:\n",
        "            return parts[-1].strip()\n",
        "\n",
        "    # ÿ•ÿ∞ÿß ŸÑŸÖ ŸÜÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ±ÿØÿå ŸÜÿ±ÿ¨ÿπ ÿßŸÑŸÜÿµ ŸÉÿßŸÖŸÑÿßŸã\n",
        "    return generated_text\n",
        "\n",
        "def rag_response(query, ranking=\"none\", k=5):\n",
        "    \"\"\"ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÉÿßŸÖŸÑ\"\"\"\n",
        "    print(f\"\\nProcessing query: '{query}'\")\n",
        "\n",
        "    # 1. ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ£ŸàŸÑŸä\n",
        "    print(\"Retrieving initial documents...\")\n",
        "    retrieved_documents = retrieve_docs(query, n=50)\n",
        "\n",
        "    # 2. ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä)\n",
        "    if ranking == \"cross_encoder\":\n",
        "        print(\"Reranking with cross-encoder...\")\n",
        "        reranked = rerank_with_cross_encoder(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    elif ranking == \"late_interaction\":\n",
        "        print(\"Reranking with late interaction...\")\n",
        "        reranked = rerank_with_late_interaction(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    else:  # ÿ®ÿØŸàŸÜ ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ\n",
        "        docs = [r['document'] for r in retrieved_documents[:k]]\n",
        "\n",
        "    # 3. ÿ™ÿ≠ÿ∂Ÿäÿ± ÿßŸÑÿ≥ŸäÿßŸÇ\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "    # 4. ÿ®ŸÜÿßÿ° ÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä\n",
        "    prompt = f\"\"\"Use the following passages to answer the question. If you don't find the answer in the passages, say \"I don't know.\"\n",
        "\n",
        "ÿßŸÑŸÖŸÇÿßÿ∑ÿπ:\n",
        "{context}\n",
        "\n",
        "ÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}\n",
        "\n",
        "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\"\"\"\n",
        "\n",
        "    # 5. ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©\n",
        "    print(\"Generating response...\")\n",
        "    return generate_local_response(prompt)\n",
        "\n",
        "# ======== ÿßŸÑŸàÿßÿ¨Ÿáÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ========\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ - ŸÖÿ∫ÿßŸÖÿ±ÿßÿ™ ÿ¥ÿ±ŸÑŸàŸÉ ŸáŸàŸÑŸÖÿ≤\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nÿ£ÿØÿÆŸÑ ÿ≥ÿ§ÿßŸÑŸÉ (ÿ£Ÿà ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿ•ŸÜŸáÿßÿ°): \")\n",
        "\n",
        "        if query.lower() in ['exit', 'quit', 'ÿÆÿ±Ÿàÿ¨']:\n",
        "            print(\"ÿ¥ŸÉÿ±ÿßŸã ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸÉ ÿßŸÑŸÜÿ∏ÿßŸÖ. ÿ•ŸÑŸâ ÿßŸÑŸÑŸÇÿßÿ°!\")\n",
        "            break\n",
        "\n",
        "        print(\"\\n1. ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\")\n",
        "        print(\"2. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Cross-Encoder)\")\n",
        "        print(\"3. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Late Interaction)\")\n",
        "        choice = input(\"ÿßÿÆÿ™ÿ± ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ®ÿ≠ÿ´ (1/2/3): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            response = rag_response(query, ranking=\"none\")\n",
        "        elif choice == '2':\n",
        "            response = rag_response(query, ranking=\"cross_encoder\")\n",
        "        elif choice == '3':\n",
        "            response = rag_response(query, ranking=\"late_interaction\")\n",
        "        else:\n",
        "            print(\"ÿßÿÆÿ™Ÿäÿßÿ± ÿ∫Ÿäÿ± ÿµÿ≠Ÿäÿ≠ÿå ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\")\n",
        "            response = rag_response(query, ranking=\"none\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\")\n",
        "        print(response)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqAuAulym_zc",
        "outputId": "21745c10-fc0e-41e4-92f9-44f81a16dd29"
      },
      "id": "JqAuAulym_zc",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All models loaded successfully!\n",
            "Populating vector database...\n",
            "Downloading Sherlock Holmes text...\n",
            "Created 1328 chunks\n",
            "Added 1328 documents to vector database\n",
            "\n",
            "==================================================\n",
            "ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ - ŸÖÿ∫ÿßŸÖÿ±ÿßÿ™ ÿ¥ÿ±ŸÑŸàŸÉ ŸáŸàŸÑŸÖÿ≤\n",
            "==================================================\n",
            "\n",
            "\n",
            "1. ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\n",
            "2. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Cross-Encoder)\n",
            "3. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Late Interaction)\n",
            "\n",
            "Processing query: 'Who is Sherlock Holmes?'\n",
            "Retrieving initial documents...\n",
            "Generating response...\n",
            "\n",
            "==================================================\n",
            "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\n",
            "[{'role': 'system', 'content': 'You are a helpful AI assistant.'}, {'role': 'user', 'content': 'ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿ≥ÿ§ÿßŸÑ. ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ¨ÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÅŸä ÿßŸÑŸÖŸÇÿßÿ∑ÿπÿå ŸÇŸÑ \"ŸÑÿß ÿ£ÿπÿ±ŸÅ\".\\n\\nÿßŸÑŸÖŸÇÿßÿ∑ÿπ:\\nDocument 1: \\n\\r\\nSherlock Holmes sat silent for some few minutes, with his brows knitted\\r\\nand his eyes fixed upon the fire. \\n\\r\\n‚ÄúDo you receive much company?‚Äù he asked. \\n\\r\\n‚ÄúNone save my partner with his family and an occasional friend of\\r\\nArthur‚Äôs. Sir George Burnwell has been several times lately. No one\\r\\nelse, I think.‚Äù\\r\\n\\r\\n‚ÄúDo you go out much in society?‚Äù\\r\\n\\r\\n‚ÄúArthur does. Mary and I stay at home.\\n\\nDocument 2: He used to make merry over the cleverness of women, but I\\r\\nhave not heard him do it of late. And when he speaks of Irene Adler, or\\r\\nwhen he refers to her photograph, it is always under the honourable\\r\\ntitle of _the_ woman. \\n\\r\\n\\r\\n\\r\\n\\r\\nII. THE RED-HEADED LEAGUE\\r\\n\\r\\n\\r\\n I had called upon my friend, Mr. Sherlock Holmes, one day in the\\r\\n autumn of last year and found him in deep conversation with a very\\r\\n stout, florid-faced, elderly gentleman with fiery red hair.\\n\\nDocument 3: Three gilt balls and a brown board with\\r\\n‚ÄúJABEZ WILSON‚Äù in white letters, upon a corner house, announced the\\r\\nplace where our red-headed client carried on his business. Sherlock\\r\\nHolmes stopped in front of it with his head on one side and looked it\\r\\nall over, with his eyes shining brightly between puckered lids. Then he\\r\\nwalked slowly up the street, and then down again to the corner, still\\r\\nlooking keenly at the houses.\\n\\nDocument 4: \\n\\r\\nSherlock Holmes‚Äô quick eye took in my occupation, and he shook his head\\r\\nwith a smile as he noticed my questioning glances. ‚ÄúBeyond the obvious\\r\\nfacts that he has at some time done manual labour, that he takes snuff,\\r\\nthat he is a Freemason, that he has been in China, and that he has done\\r\\na considerable amount of writing lately, I can deduce nothing else.‚Äù\\r\\n\\r\\nMr. Jabez Wilson started up in his chair, with his forefinger upon the\\r\\npaper, but his eyes upon my companion.\\n\\nDocument 5: \\ufeffThe Project Gutenberg eBook of The Adventures of Sherlock Holmes,\\r\\nby Arthur Conan Doyle\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org.\\n\\nÿßŸÑÿ≥ÿ§ÿßŸÑ: Who is Sherlock Holmes?\\n\\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:'}, {'role': 'assistant', 'content': 'ÿßŸÑŸÖŸÇÿßÿ∑ÿπ:\\n\\n sherlock holmes sat silent for some few minutes, with his brows knitted and his eyes fixed upon the fire. \\n\\n‚ÄúDo you receive much company?‚Äù he asked.\\n\\n‚ÄúNone save my partner with his family and an occasional friend of Arthur‚Äôs. Sir George Burnwell has been several times lately. No one else, I think.‚Äù\\n\\n‚ÄúDo you go out much in society?‚Äù\\n\\n‚ÄúArthur does. Mary and I stay at home.\\n\\nsherlock holmes sat silent for some few minutes, with his brows knitted and his eyes fixed upon the fire. \\n\\n‚ÄúDo you go out much in society?‚Äù\\n\\n‚ÄúArthur does. Mary and I stay at home.\\n\\n sherlock holmes sat silent for some few minutes, with his brows knitted and his eyes fixed upon the fire. \\n\\n‚ÄúDo you go out much in society?‚Äù\\n\\n‚ÄúArthur does. Mary and I stay at home.\\n\\nsherlock holmes sat silent for some few minutes, with his brows knitted and his eyes fixed upon the fire. \\n\\n‚ÄúDo you go out much in society?‚Äù\\n\\n‚ÄúArthur does. Mary and I stay at home.'}]\n",
            "==================================================\n",
            "\n",
            "ÿ£ÿØÿÆŸÑ ÿ≥ÿ§ÿßŸÑŸÉ (ÿ£Ÿà ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿ•ŸÜŸáÿßÿ°): What are the adventures of Sherlock Holmes?\n",
            "\n",
            "1. ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\n",
            "2. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Cross-Encoder)\n",
            "3. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Late Interaction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ŸÜÿ∏ÿßŸÖ RAG ŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖÿßÿ∞ÿ¨ ŸÖÿ¨ÿßŸÜŸäÿ© - ÿßŸÑÿ•ÿµÿØÿßÿ± ÿßŸÑŸÖÿπÿØŸÑ\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "import chromadb\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ======== ÿ•ÿπÿØÿßÿØ ÿßŸÑÿ®Ÿäÿ¶ÿ© ========\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ ŸÑŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ Cross-Encoder\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ ColBERT\n",
        "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# ŸÜŸÖŸàÿ∞ÿ¨ ÿ™ŸàŸÑŸäÿØ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑŸÖÿ≠ŸÑŸä (ÿ®ÿØŸäŸÑ ÿ£ÿµÿ∫ÿ±: 'gpt2')\n",
        "local_llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "print(\"All models loaded successfully!\")\n",
        "\n",
        "# ======== ÿ•ÿπÿØÿßÿØ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ© ========\n",
        "def split_sentences(text):\n",
        "    \"\"\"ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ¨ŸÖŸÑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ regex ÿ®ÿØŸàŸÜ ÿßŸÑÿ≠ÿßÿ¨ÿ© ŸÑŸÄ nltk\"\"\"\n",
        "    # ÿ™ŸÇÿ≥ŸäŸÖ ÿπŸÜÿØ ÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ™ÿ±ŸÇŸäŸÖ ŸÖÿπ ÿßŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ÿßŸÑÿßÿÆÿ™ÿµÿßÿ±ÿßÿ™\n",
        "    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "    return re.split(pattern, text)\n",
        "\n",
        "def prepare_documents():\n",
        "    \"\"\"ÿ™ÿ≠ÿ∂Ÿäÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ŸÑŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ©\"\"\"\n",
        "    print(\"Downloading Sherlock Holmes text...\")\n",
        "    url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "\n",
        "    # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÜÿµ ŸÖŸÜ ÿßŸÑÿ±ÿ£ÿ≥ ŸàÿßŸÑÿ™ÿ∞ŸäŸäŸÑ\n",
        "    start_idx = text.find(\"ADVENTURE I. A SCANDAL IN BOHEMIA\")\n",
        "    end_idx = text.find(\"End of the Project Gutenberg\")\n",
        "    text = text[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else text\n",
        "\n",
        "    # ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ¨ŸÖŸÑ\n",
        "    sentences = split_sentences(text)\n",
        "\n",
        "    # ÿ™ÿ¨ŸÖŸäÿπ ÿßŸÑÿ¨ŸÖŸÑ ŸÅŸä ŸÖŸÇÿßÿ∑ÿπ\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    token_limit = 400  # ÿ≠ÿØ ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑŸÖÿ≥ÿ™ŸáÿØŸÅ\n",
        "    char_per_token = 4  # ÿ™ŸÇÿØŸäÿ±: 4 ÿ£ÿ≠ÿ±ŸÅ ŸÑŸÉŸÑ ÿ±ŸÖÿ≤\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿä\n",
        "        sentence_tokens = len(sentence) / char_per_token\n",
        "\n",
        "        if len(current_chunk) + sentence_tokens > token_limit:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "            current_chunk = sentence\n",
        "        else:\n",
        "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# ÿ™ŸáŸäÿ¶ÿ© ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ™ÿ¨Ÿáÿ©\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"sherlock_holmes\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ŸÉŸÜ ŸÖŸàÿ¨ŸàÿØÿ©\n",
        "if collection.count() == 0:\n",
        "    print(\"Populating vector database...\")\n",
        "    chunks = prepare_documents()\n",
        "    embeddings = embedding_model.encode(chunks).tolist()\n",
        "    ids = [str(i) for i in range(len(chunks))]\n",
        "\n",
        "    collection.add(\n",
        "        documents=chunks,\n",
        "        embeddings=embeddings,\n",
        "        ids=ids\n",
        "    )\n",
        "    print(f\"Added {len(chunks)} documents to vector database\")\n",
        "else:\n",
        "    print(f\"Vector database already contains {collection.count()} documents\")\n",
        "\n",
        "# ======== ÿØŸàÿßŸÑ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ========\n",
        "def get_token_embeddings(text, tokenizer, model):\n",
        "    \"\"\"ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿ™ŸÖÿ´ŸäŸÑÿßÿ™ ÿßŸÑÿ±ŸÖŸàÿ≤\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    # ÿ™ÿ¨ÿßŸáŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ÿßŸÑÿÆÿßÿµÿ© [CLS] Ÿà [SEP]\n",
        "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
        "    return outputs.last_hidden_state[0][keep_indices]\n",
        "\n",
        "def colbert_score(query_emb, doc_emb):\n",
        "    \"\"\"ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ColBERT\"\"\"\n",
        "    # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜ ŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ŸàŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ\n",
        "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)\n",
        "    # ÿ£ÿÆÿ∞ ÿ£ŸÇÿµŸâ ÿ™ÿ¥ÿßÿ®Ÿá ŸÑŸÉŸÑ ÿ±ŸÖÿ≤ ŸÅŸä ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    max_sim, _ = sim.max(dim=1)\n",
        "    # ÿ¨ŸÖÿπ ÿßŸÑŸÇŸäŸÖ ÿßŸÑŸÇÿµŸàŸâ\n",
        "    return max_sim.sum().item()\n",
        "\n",
        "# ======== ÿØŸàÿßŸÑ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ Ÿàÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ ========\n",
        "def retrieve_docs(query, n=25):\n",
        "    \"\"\"ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ£ŸàŸÑŸä ŸÖŸÜ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\"\"\"\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n\n",
        "    )\n",
        "    return [\n",
        "        {\"document\": doc, \"score\": score}\n",
        "        for doc, score in zip(results['documents'][0], results['distances'][0])\n",
        "    ]\n",
        "\n",
        "def rerank_with_cross_encoder(query, results):\n",
        "    \"\"\"ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ Cross-Encoder\"\"\"\n",
        "    documents = [r['document'] for r in results]\n",
        "    # ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿßÿ™ ÿßŸÑÿ£ŸáŸÖŸäÿ© ŸÑŸÉŸÑ ŸÖÿ≥ÿ™ŸÜÿØ ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    rerank_scores = cross_encoder.predict([(query, doc) for doc in documents])\n",
        "    for r, score in zip(results, rerank_scores):\n",
        "        r['cross_encoder_score'] = float(score)\n",
        "    # ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ±ÿ¨ÿ©\n",
        "    return sorted(results, key=lambda x: x['cross_encoder_score'], reverse=True)\n",
        "\n",
        "def rerank_with_late_interaction(query, results):\n",
        "    \"\"\"ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ™ŸÅÿßÿπŸÑ ÿßŸÑŸÖÿ™ÿ£ÿÆÿ± (ColBERT)\"\"\"\n",
        "    # ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÑŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ\n",
        "    query_emb = get_token_embeddings(query, colbert_tokenizer, colbert_model)\n",
        "    for r in results:\n",
        "        # ÿ™ŸÖÿ´ŸäŸÑ ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÑŸÉŸÑ ŸÖÿ≥ÿ™ŸÜÿØ\n",
        "        doc_emb = get_token_embeddings(r['document'], colbert_tokenizer, colbert_model)\n",
        "        # ÿ≠ÿ≥ÿßÿ® ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸÅÿßÿπŸÑ ÿßŸÑŸÖÿ™ÿ£ÿÆÿ±\n",
        "        r['late_interaction_score'] = colbert_score(query_emb, doc_emb)\n",
        "    # ÿ™ÿ±ÿ™Ÿäÿ® ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿ™ŸÜÿßÿ≤ŸÑŸäÿßŸã ÿ≠ÿ≥ÿ® ÿßŸÑÿØÿ±ÿ¨ÿ©\n",
        "    return sorted(results, key=lambda x: x['late_interaction_score'], reverse=True)\n",
        "\n",
        "# ======== ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ========\n",
        "def generate_local_response(prompt, max_length=500):\n",
        "    \"\"\"ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ŸÖÿ≠ŸÑŸä\"\"\"\n",
        "    # ÿ®ŸÜÿßÿ° ÿ±ÿ≥ÿßÿ¶ŸÑ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿ©\n",
        "    response = local_llm(\n",
        "        messages,\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=local_llm.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ©\n",
        "    generated_text = response[0]['generated_text']\n",
        "\n",
        "    # ŸÅŸä ÿ®ÿπÿ∂ ÿßŸÑÿ£ÿ≠ŸäÿßŸÜ Ÿäÿ±ÿ¨ÿπ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ±ÿ≥ÿßÿ¶ŸÑ ŸÉÿßŸÖŸÑÿ©ÿå ŸÑÿ∞ŸÑŸÉ ŸÜÿ≥ÿ™ÿÆÿ±ÿ¨ ŸÅŸÇÿ∑ ÿßŸÑÿ±ÿØ ÿßŸÑÿ£ÿÆŸäÿ±\n",
        "    if \"assistant\" in generated_text:\n",
        "        # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ¨ÿ≤ÿ° ÿ®ÿπÿØ ÿ¢ÿÆÿ± \"assistant\"\n",
        "        parts = generated_text.split(\"assistant\")\n",
        "        if len(parts) > 1:\n",
        "            return parts[-1].strip()\n",
        "\n",
        "    # ÿ•ÿ∞ÿß ŸÑŸÖ ŸÜÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ±ÿØÿå ŸÜÿ±ÿ¨ÿπ ÿßŸÑŸÜÿµ ŸÉÿßŸÖŸÑÿßŸã\n",
        "    return generated_text\n",
        "\n",
        "def rag_response(query, ranking=\"none\", k=5):\n",
        "    \"\"\"ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÉÿßŸÖŸÑ\"\"\"\n",
        "    print(f\"\\nProcessing query: '{query}'\")\n",
        "\n",
        "    # 1. ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ£ŸàŸÑŸä\n",
        "    print(\"Retrieving initial documents...\")\n",
        "    retrieved_documents = retrieve_docs(query, n=50)\n",
        "\n",
        "    # 2. ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä)\n",
        "    if ranking == \"cross_encoder\":\n",
        "        print(\"Reranking with cross-encoder...\")\n",
        "        reranked = rerank_with_cross_encoder(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    elif ranking == \"late_interaction\":\n",
        "        print(\"Reranking with late interaction...\")\n",
        "        reranked = rerank_with_late_interaction(query, retrieved_documents)\n",
        "        docs = [r['document'] for r in reranked[:k]]\n",
        "    else:  # ÿ®ÿØŸàŸÜ ÿ•ÿπÿßÿØÿ© ÿ™ÿµŸÜŸäŸÅ\n",
        "        docs = [r['document'] for r in retrieved_documents[:k]]\n",
        "\n",
        "    # 3. ÿ™ÿ≠ÿ∂Ÿäÿ± ÿßŸÑÿ≥ŸäÿßŸÇ\n",
        "    context = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "    # 4. ÿ®ŸÜÿßÿ° ÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä\n",
        "    prompt = f\"\"\"Use the following passages to answer the question. If you don't find the answer in the passages, say \"I don't know.\"\n",
        "\n",
        "ÿßŸÑŸÖŸÇÿßÿ∑ÿπ:\n",
        "{context}\n",
        "\n",
        "ÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}\n",
        "\n",
        "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\"\"\"\n",
        "\n",
        "    # 5. ÿ™ŸàŸÑŸäÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©\n",
        "    print(\"Generating response...\")\n",
        "    return generate_local_response(prompt)\n",
        "\n",
        "# ======== ÿßŸÑŸàÿßÿ¨Ÿáÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ========\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ - ŸÖÿ∫ÿßŸÖÿ±ÿßÿ™ ÿ¥ÿ±ŸÑŸàŸÉ ŸáŸàŸÑŸÖÿ≤\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nÿ£ÿØÿÆŸÑ ÿ≥ÿ§ÿßŸÑŸÉ (ÿ£Ÿà ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿ•ŸÜŸáÿßÿ°): \")\n",
        "\n",
        "        if query.lower() in ['exit', 'quit', 'ÿÆÿ±Ÿàÿ¨']:\n",
        "            print(\"ÿ¥ŸÉÿ±ÿßŸã ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸÉ ÿßŸÑŸÜÿ∏ÿßŸÖ. ÿ•ŸÑŸâ ÿßŸÑŸÑŸÇÿßÿ°!\")\n",
        "            break\n",
        "\n",
        "        print(\"\\n1. ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\")\n",
        "        print(\"2. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Cross-Encoder)\")\n",
        "        print(\"3. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Late Interaction)\")\n",
        "        choice = input(\"ÿßÿÆÿ™ÿ± ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ®ÿ≠ÿ´ (1/2/3): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            response = rag_response(query, ranking=\"none\")\n",
        "        elif choice == '2':\n",
        "            response = rag_response(query, ranking=\"cross_encoder\")\n",
        "        elif choice == '3':\n",
        "            response = rag_response(query, ranking=\"late_interaction\")\n",
        "        else:\n",
        "            print(\"ÿßÿÆÿ™Ÿäÿßÿ± ÿ∫Ÿäÿ± ÿµÿ≠Ÿäÿ≠ÿå ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\")\n",
        "            response = rag_response(query, ranking=\"none\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\")\n",
        "        print(response)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK-yX9JUzKIn",
        "outputId": "3e869d7c-b756-425a-d3a9-0f0267425b26"
      },
      "id": "YK-yX9JUzKIn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All models loaded successfully!\n",
            "Vector database already contains 1328 documents\n",
            "\n",
            "==================================================\n",
            "ŸÜÿ∏ÿßŸÖ RAG ÿßŸÑŸÖÿ≠ŸÑŸä ÿ®ÿßŸÑŸÉÿßŸÖŸÑ - ŸÖÿ∫ÿßŸÖÿ±ÿßÿ™ ÿ¥ÿ±ŸÑŸàŸÉ ŸáŸàŸÑŸÖÿ≤\n",
            "==================================================\n",
            "\n",
            "\n",
            "ÿ£ÿØÿÆŸÑ ÿ≥ÿ§ÿßŸÑŸÉ (ÿ£Ÿà ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿ•ŸÜŸáÿßÿ°): What is the name of Sherlock's friend?\n",
            "\n",
            "1. ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä ŸÅŸÇÿ∑\n",
            "2. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Cross-Encoder)\n",
            "3. ÿßŸÑÿ®ÿ≠ÿ´ ŸÖÿπ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ÿµŸÜŸäŸÅ (Late Interaction)\n",
            "ÿßÿÆÿ™ÿ± ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑÿ®ÿ≠ÿ´ (1/2/3): 1\n",
            "\n",
            "Processing query: 'What is the name of Sherlock's friend?'\n",
            "Retrieving initial documents...\n",
            "Generating response...\n",
            "\n",
            "==================================================\n",
            "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\n",
            "[{'role': 'system', 'content': 'You are a helpful AI assistant.'}, {'role': 'user', 'content': 'Use the following passages to answer the question. If you don\\'t find the answer in the passages, say \"I don\\'t know.\"\\n\\nÿßŸÑŸÖŸÇÿßÿ∑ÿπ:\\nDocument 1: \\n\\r\\nSherlock Holmes sat silent for some few minutes, with his brows knitted\\r\\nand his eyes fixed upon the fire. \\n\\r\\n‚ÄúDo you receive much company?‚Äù he asked. \\n\\r\\n‚ÄúNone save my partner with his family and an occasional friend of\\r\\nArthur‚Äôs. Sir George Burnwell has been several times lately. No one\\r\\nelse, I think.‚Äù\\r\\n\\r\\n‚ÄúDo you go out much in society?‚Äù\\r\\n\\r\\n‚ÄúArthur does. Mary and I stay at home.\\n\\nDocument 2: He used to make merry over the cleverness of women, but I\\r\\nhave not heard him do it of late. And when he speaks of Irene Adler, or\\r\\nwhen he refers to her photograph, it is always under the honourable\\r\\ntitle of _the_ woman. \\n\\r\\n\\r\\n\\r\\n\\r\\nII. THE RED-HEADED LEAGUE\\r\\n\\r\\n\\r\\n I had called upon my friend, Mr. Sherlock Holmes, one day in the\\r\\n autumn of last year and found him in deep conversation with a very\\r\\n stout, florid-faced, elderly gentleman with fiery red hair.\\n\\nDocument 3: How could you know anything of the matter?‚Äù\\r\\n\\r\\n‚ÄúMy name is Sherlock Holmes. It is my business to know what other\\r\\npeople don‚Äôt know.‚Äù\\r\\n\\r\\n‚ÄúBut you can know nothing of this?‚Äù\\r\\n\\r\\n‚ÄúExcuse me, I know everything of it. You are endeavouring to trace some\\r\\ngeese which were sold by Mrs. Oakshott, of Brixton Road, to a salesman\\r\\nnamed Breckinridge, by him in turn to Mr. Windigate, of the Alpha, and\\r\\nby him to his club, of which Mr. Henry Baker is a member.‚Äù\\r\\n\\r\\n‚ÄúOh, sir, you are the very man whom I have longed to meet,‚Äù cried the\\r\\nlittle fellow with outstretched hands and quivering fingers.\\n\\nDocument 4: I rapidly threw on\\r\\nmy clothes and was ready in a few minutes to accompany my friend down\\r\\nto the sitting-room. A lady dressed in black and heavily veiled, who\\r\\nhad been sitting in the window, rose as we entered. \\n\\r\\n‚ÄúGood-morning, madam,‚Äù said Holmes cheerily. ‚ÄúMy name is Sherlock\\r\\nHolmes. This is my intimate friend and associate, Dr. Watson, before\\r\\nwhom you can speak as freely as before myself. Ha!\\n\\nDocument 5: Three gilt balls and a brown board with\\r\\n‚ÄúJABEZ WILSON‚Äù in white letters, upon a corner house, announced the\\r\\nplace where our red-headed client carried on his business. Sherlock\\r\\nHolmes stopped in front of it with his head on one side and looked it\\r\\nall over, with his eyes shining brightly between puckered lids. Then he\\r\\nwalked slowly up the street, and then down again to the corner, still\\r\\nlooking keenly at the houses.\\n\\nÿßŸÑÿ≥ÿ§ÿßŸÑ: What is the name of Sherlock\\'s friend?\\n\\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:'}, {'role': 'assistant', 'content': \"The name of Sherlock's friend is Dr. Watson.\"}]\n",
            "==================================================\n",
            "\n",
            "ÿ£ÿØÿÆŸÑ ÿ≥ÿ§ÿßŸÑŸÉ (ÿ£Ÿà ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿ•ŸÜŸáÿßÿ°): ÿÆÿ±Ÿàÿ¨\n",
            "ÿ¥ŸÉÿ±ÿßŸã ŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸÉ ÿßŸÑŸÜÿ∏ÿßŸÖ. ÿ•ŸÑŸâ ÿßŸÑŸÑŸÇÿßÿ°!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do you think that Neville is alive?\n",
        "What is the name of Sherlock's friend?\n",
        "What is Sherlock's family name?\n",
        "Tell me the names of Sherlock's adventures"
      ],
      "metadata": {
        "id": "tf9rrqQtnHbc"
      },
      "id": "tf9rrqQtnHbc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e136f3d025ce4763991d4cc1e65f6ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ad160e232454251b5c9697e14dbc296",
              "IPY_MODEL_947c7d1434614f3cb94f0748b3268810",
              "IPY_MODEL_ccae06879ecb43a38bd2161d5e9c75de"
            ],
            "layout": "IPY_MODEL_df35847a75ba4b2d9949b6b8b67f614b"
          }
        },
        "2ad160e232454251b5c9697e14dbc296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d43f2c7c734404baa5cfbb4d0642881",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_79568d1129854e82a86373d34c380211",
            "value": "config.json:‚Äá100%"
          }
        },
        "947c7d1434614f3cb94f0748b3268810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b3ca868c8dd4fef87b495bc7d0e1e07",
            "max": 877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04fb696816b541bb986a46e6b9e1e65e",
            "value": 877
          }
        },
        "ccae06879ecb43a38bd2161d5e9c75de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d601d5b523d4fb2a576aff1dd8a26ce",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6fc53934598a4f0dbd698df039e3c07a",
            "value": "‚Äá877/877‚Äá[00:00&lt;00:00,‚Äá40.1kB/s]"
          }
        },
        "df35847a75ba4b2d9949b6b8b67f614b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d43f2c7c734404baa5cfbb4d0642881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79568d1129854e82a86373d34c380211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b3ca868c8dd4fef87b495bc7d0e1e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04fb696816b541bb986a46e6b9e1e65e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d601d5b523d4fb2a576aff1dd8a26ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc53934598a4f0dbd698df039e3c07a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b116c6c49fba4cd68f1119d761518d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b92da0dc70b24244b31f9098ae125fae",
              "IPY_MODEL_cd95485c324b43f5ac673b6c76e79bb4",
              "IPY_MODEL_8e0d8439c49a4e57b70797a5f7bcc932"
            ],
            "layout": "IPY_MODEL_a078a780e2f24c40ae1e7fe377228e99"
          }
        },
        "b92da0dc70b24244b31f9098ae125fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_702f00bbacda4acfb4ebf5b8c8467fac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ced0b9e49f5046ebbf455e738f35e238",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "cd95485c324b43f5ac673b6c76e79bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a69a37383145ed8b95247b8e819a1a",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4913fc42ae64a3cb215536e0fb4c0c7",
            "value": 2471645608
          }
        },
        "8e0d8439c49a4e57b70797a5f7bcc932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fddb9d7408e4363bc2a00c42a472829",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0cc5b82eb12840d6a0991f1f1f80b855",
            "value": "‚Äá2.47G/2.47G‚Äá[00:44&lt;00:00,‚Äá35.8MB/s]"
          }
        },
        "a078a780e2f24c40ae1e7fe377228e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702f00bbacda4acfb4ebf5b8c8467fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced0b9e49f5046ebbf455e738f35e238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a69a37383145ed8b95247b8e819a1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4913fc42ae64a3cb215536e0fb4c0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fddb9d7408e4363bc2a00c42a472829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc5b82eb12840d6a0991f1f1f80b855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b78af48c25e84b18a6b2611ff4134f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a16023bcfa94c0ca5aba222dd80c9bf",
              "IPY_MODEL_db0b6cba8269404aa1ae0683f2c65014",
              "IPY_MODEL_0193ca6d6af74d0a883a1ad7a2782ff1"
            ],
            "layout": "IPY_MODEL_86986ba1e8ff4f93ac57833e52e379d8"
          }
        },
        "4a16023bcfa94c0ca5aba222dd80c9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bceab284519b458bbe5b28e02a00ad68",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_34f520a82abd4b4ab4474112c8051c3c",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "db0b6cba8269404aa1ae0683f2c65014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_323524c7db964f338efe25959dfdcba5",
            "max": 189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd908e53aebb4a46936b3d86abb00bda",
            "value": 189
          }
        },
        "0193ca6d6af74d0a883a1ad7a2782ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aacfb69a64f499ea0fe5edca10f2bfd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_78062b09f15946db8104dcc13253ce13",
            "value": "‚Äá189/189‚Äá[00:00&lt;00:00,‚Äá14.5kB/s]"
          }
        },
        "86986ba1e8ff4f93ac57833e52e379d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bceab284519b458bbe5b28e02a00ad68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34f520a82abd4b4ab4474112c8051c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "323524c7db964f338efe25959dfdcba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd908e53aebb4a46936b3d86abb00bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aacfb69a64f499ea0fe5edca10f2bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78062b09f15946db8104dcc13253ce13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b207c440458435abdae79587e2b6ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_342959cca23544daaf2bc0e18da8b39e",
              "IPY_MODEL_c1a2f4d593924bd7ad01f958fd8ea2b0",
              "IPY_MODEL_19f967b307774bf2947186a532dc47fa"
            ],
            "layout": "IPY_MODEL_9be2adc0b9e3410c89d8f8dc1fc4e640"
          }
        },
        "342959cca23544daaf2bc0e18da8b39e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cd1e86925c9476dbd76f54176b28fb0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d7e3e934eb941358e3dc1ef18452404",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "c1a2f4d593924bd7ad01f958fd8ea2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_876a7446ad3d497a8d3ab8380a8c69f7",
            "max": 54528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba1072c1945843d9850f02e194722aaa",
            "value": 54528
          }
        },
        "19f967b307774bf2947186a532dc47fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330cad69f5be4f95af011eaae7a16390",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_322d23198e894028859bc998be22a11b",
            "value": "‚Äá54.5k/54.5k‚Äá[00:00&lt;00:00,‚Äá4.35MB/s]"
          }
        },
        "9be2adc0b9e3410c89d8f8dc1fc4e640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd1e86925c9476dbd76f54176b28fb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7e3e934eb941358e3dc1ef18452404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "876a7446ad3d497a8d3ab8380a8c69f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba1072c1945843d9850f02e194722aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "330cad69f5be4f95af011eaae7a16390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "322d23198e894028859bc998be22a11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2841f3e1871e4c14aaf9ef8f729dd963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff07c2a9e54e446ab4d16e04a649b92b",
              "IPY_MODEL_b79f5d7439144c8fae076e653dc4ace2",
              "IPY_MODEL_4a0c099bdd414c06b49b9679c07cb9b8"
            ],
            "layout": "IPY_MODEL_fac4df09db8a476ab962267f917b3140"
          }
        },
        "ff07c2a9e54e446ab4d16e04a649b92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aec63b14d3a46ffb3cc6fd2f5de3a8e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e59f7244190e457cb41ce43b5c90e298",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "b79f5d7439144c8fae076e653dc4ace2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73a31aa0dc834f67a4d9ad60e53166fe",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27bd08b30c8c49b1acfaa43a88c336eb",
            "value": 9085657
          }
        },
        "4a0c099bdd414c06b49b9679c07cb9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4652e998c2024182a903d49858e165c6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ada1fed3b8b7430db7a7d498283eac96",
            "value": "‚Äá9.09M/9.09M‚Äá[00:00&lt;00:00,‚Äá25.6MB/s]"
          }
        },
        "fac4df09db8a476ab962267f917b3140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aec63b14d3a46ffb3cc6fd2f5de3a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59f7244190e457cb41ce43b5c90e298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73a31aa0dc834f67a4d9ad60e53166fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bd08b30c8c49b1acfaa43a88c336eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4652e998c2024182a903d49858e165c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada1fed3b8b7430db7a7d498283eac96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac06b46dfce45a7a7764042b4705139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cff81a6d1b9b460f9e539c2d22c3ba98",
              "IPY_MODEL_5cf5fb7afa3e4696af0a6b23b63a7d76",
              "IPY_MODEL_b564ac9180d5432f9e664b216cf0c5a2"
            ],
            "layout": "IPY_MODEL_d06132c3059f4d3384df2ea27e6d90cc"
          }
        },
        "cff81a6d1b9b460f9e539c2d22c3ba98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b83be3ea414ca899b4e59477377a18",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a25869acc86841f2a1511b4574ce3761",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "5cf5fb7afa3e4696af0a6b23b63a7d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a061f25eb82743428b7cd5ed927814a9",
            "max": 296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_977a73d9d7c443c09d50fd9913fb39f0",
            "value": 296
          }
        },
        "b564ac9180d5432f9e664b216cf0c5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b4855a29c894736992ef7e981887338",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f1a0a86b7bcf4cec871ca0322211bdc0",
            "value": "‚Äá296/296‚Äá[00:00&lt;00:00,‚Äá17.7kB/s]"
          }
        },
        "d06132c3059f4d3384df2ea27e6d90cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b83be3ea414ca899b4e59477377a18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a25869acc86841f2a1511b4574ce3761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a061f25eb82743428b7cd5ed927814a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "977a73d9d7c443c09d50fd9913fb39f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b4855a29c894736992ef7e981887338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a0a86b7bcf4cec871ca0322211bdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}